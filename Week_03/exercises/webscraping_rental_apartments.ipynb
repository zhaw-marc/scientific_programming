{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Rental Apartment Data with Python and Beautifulsoup\n",
    "\n",
    "Data from: \n",
    "- https://www.immoscout24.ch/de/immobilien/mieten/ort-zuerich?pn=1  \n",
    "- https://www.immoscout24.ch/de/immobilien/mieten/ort-zuerich?pn=2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of scraping data from websites with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HTML from local files (offline) instead of fetching the website\n",
    "html_paths = [\n",
    "    Path('./data/immo_listings_pn_01.html'),\n",
    "    Path('./data/immo_listings_pn_02.html'),\n",
    "].copy()\n",
    "\n",
    "html_texts = [p.read_text(encoding='utf-8') for p in html_paths]\n",
    "soups = [BeautifulSoup(t, 'html.parser') for t in html_texts]\n",
    "\n",
    "# Extract listing blocks from each file (hashed class names -> match by substring)\n",
    "listings = []\n",
    "for p, s in zip(html_paths, soups):\n",
    "    listings = s.select('div[class*=\"HgListingRoomsLivingSpacePrice_roomsLivingSpacePrice_\"]')\n",
    "    print(f\"{p.name}: {len(listings)} listing blocks\")\n",
    "    listings.extend(listings)\n",
    "\n",
    "print('Total listing blocks:', len(listings))\n",
    "\n",
    "# Show first 10 listing blocks (from combined pages)\n",
    "listings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: check whether we loaded normal HTML files\n",
    "for p, t, s in zip(html_paths, html_texts, soups):\n",
    "    print('Source:', p.resolve())\n",
    "    print('Length HTML:', len(t))\n",
    "    print('Title:', s.title.get_text(strip=True) if s.title else None)\n",
    "    sample = t[:400].replace('\\n', ' ')\n",
    "    print('HTML sample:', sample)\n",
    "    print('Contains HgListingRoomsLivingSpacePrice:', 'HgListingRoomsLivingSpacePrice' in t)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-parse the saved HTML files to show it's consistent\n",
    "for p, t in zip(html_paths, html_texts):\n",
    "    soup_check = BeautifulSoup(t, 'html.parser')\n",
    "    print(p.name, '| title:', soup_check.title.get_text(strip=True) if soup_check.title else None, '| length:', len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional note: If you scrape the live site, you may need a real browser engine (JS rendering / bot checks).\n",
    "# In this exercise we run fully offline from ./data/immo_listings.html.\n",
    "print('Offline mode: using local HTML file; browser rendering step skipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the listings from a given URL OR a local HTML file\n",
    "def extract_listing_info(source):\n",
    "    source_path = Path(str(source))\n",
    "    if source_path.suffix.lower() in ('.html', '.htm') and source_path.exists():\n",
    "        html = source_path.read_text(encoding='utf-8')\n",
    "    else:\n",
    "        response = requests.get(source)\n",
    "        response.raise_for_status()\n",
    "        html = response.text\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Extract the listing cards (hashed class suffix changes)\n",
    "    listings = soup.select('div[class*=\"HgListingCard_info_\"]')\n",
    "    \n",
    "    extracted_data = []\n",
    "    for listing in listings:\n",
    "        # Extract address\n",
    "        address_tag = listing.find('address')\n",
    "        address = address_tag.get_text(strip=True) if address_tag else 'N/A'\n",
    "\n",
    "        # Extract rooms, living area, and price\n",
    "        info_section = listing.select_one('div[class*=\"HgListingRoomsLivingSpacePrice_roomsLivingSpacePrice_\"]')\n",
    "        if info_section:\n",
    "            info_text = [strong.get_text(strip=True) for strong in info_section.find_all('strong')]\n",
    "            rooms = info_text[0] if len(info_text) > 0 else 'N/A'\n",
    "            living_area = info_text[1] if len(info_text) > 1 else 'N/A'\n",
    "            price_tag = info_section.select_one('span[class*=\"HgListingRoomsLivingSpacePrice_price_\"]')\n",
    "            price = price_tag.get_text(strip=True) if price_tag else 'N/A'\n",
    "        else:\n",
    "            rooms, living_area, price = 'N/A', 'N/A', 'N/A'\n",
    "\n",
    "        # Extract title and description\n",
    "        title_tag = listing.select_one('p[class*=\"HgListingDescription_title_\"]')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
    "\n",
    "        description_tag = listing.select_one('p[class*=\"HgListingDescription_extra-large_\"], p[class*=\"HgListingDescription_large_\"]')\n",
    "        description = description_tag.get_text(strip=True) if description_tag else 'N/A'\n",
    "\n",
    "        extracted_data.append({\n",
    "            'address_raw': address,\n",
    "            'rooms_raw': rooms,\n",
    "            'area_raw': living_area,\n",
    "            'price_raw': price,\n",
    "            'title_raw': title,\n",
    "            'description_raw': description\n",
    "        })\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Web Scraper function to get apartment data from the 1st page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse multiple saved HTML pages (offline)\n",
    "sources = [\n",
    "    Path('./data/immo_listings_pn_01.html'),\n",
    "    Path('./data/immo_listings_pn_02.html'),\n",
    "].copy()\n",
    "\n",
    "# Extract listing information from both pages\n",
    "data = []\n",
    "for src in sources:\n",
    "    data.extend(extract_listing_info(src))\n",
    "\n",
    "# Create DataFrame from combined data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the Web Scraper function to get apartment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline version: parse multiple saved HTML pages\n",
    "sources = [\n",
    "    Path('./data/immo_listings_pn_01.html'),\n",
    "    Path('./data/immo_listings_pn_02.html'),\n",
    "].copy()\n",
    "\n",
    "data = []\n",
    "for src in sources:\n",
    "    data.extend(extract_listing_info(src))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('./data/rental_apartments_zuerich.csv', index=False)\n",
    "\n",
    "print(\"\\nData saved to rental_apartments_zuerich.csv\")\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract prices from strings using a regular expression (regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract prices from the 'price_raw' column using a regular expression (regex)\n",
    "df['price'] = df['price_raw'].str.extract(r'(\\d[\\d’]*\\d)')\n",
    "\n",
    "# Remove thousands separator and convert to float\n",
    "df['price'] = df['price'].str.replace('’', '').astype(float)\n",
    "\n",
    "# Show the first few rows of the DataFrame (selected columns)\n",
    "df[['address_raw', 'rooms_raw', 'area_raw', 'price_raw', 'price']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create histogram of prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the prices\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(df['price'], bins=30, color='greenyellow', edgecolor='gray')\n",
    "plt.title('Histogram of prices')\n",
    "plt.xlabel('Price (CHF)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
