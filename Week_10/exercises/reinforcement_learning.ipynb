{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object oriented programming (OOP): Reinforcement Learning\n",
    "\n",
    "Key Objectives:\n",
    "- Introduce Core Reinforcement Learning (RL) Concepts: States, actions, rewards, policies, and value functions.\n",
    "- Implement a Simple Environment: Grid World is used as a basic environment to simplify visualization and understanding of how agents learn.\n",
    "- Apply Value Iteration or Policy Iteration: The model uses value iteration to learn an optimal policy.\n",
    "- Learn an Optimal Policy: The agent learns the best action to take in each state in order to maximize cumulative reward over time.\n",
    "\n",
    "Example taken from: https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global variables\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 4\n",
    "WIN_STATE = (0, 3)\n",
    "LOSE_STATE = (1, 3)\n",
    "START = (2, 0)\n",
    "DETERMINISTIC = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule for the agent\n",
    "\n",
    "Your agent starts at the left-bottom corner (the 'start' sign) and ends at either +1 or -1 which is the corresponding reward. At each step, the agent has 4 possible actions including up, down, left and right, whereas the black cell is a wall where your agent wonâ€™t be able to penetrate through. In order to make it more straight forward, our first implementation assumes that each action is deterministic, that is, the agent will go where it intends to go. For instance, when the agent decides to take action up at (2, 0), it will land in (1, 0) rather than (2, 1) or elsewhere. (We will add uncertainty in out second implementation) However, it the agents hit the wall, it will remain at the same position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the board with zeros\n",
    "board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "\n",
    "# Mark the start, win, and lose states on the grid\n",
    "board[START] = 0.5\n",
    "board[WIN_STATE] = 1\n",
    "board[LOSE_STATE] = -1\n",
    "\n",
    "# Block cell\n",
    "block_state = (1, 1)\n",
    "board[block_state] = np.nan  # Use NaN for blocked cells\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Use a heatmap to show the different states\n",
    "cmap = plt.cm.coolwarm\n",
    "cmap.set_bad('black', 1.)\n",
    "heatmap = ax.imshow(board, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "# Add text in each cell\n",
    "for i in range(BOARD_ROWS):\n",
    "    for j in range(BOARD_COLS):\n",
    "        if (i, j) == WIN_STATE:\n",
    "            text = ax.text(j, i, '+1', ha='center', va='center', color='white')\n",
    "        elif (i, j) == LOSE_STATE:\n",
    "            text = ax.text(j, i, '-1', ha='center', va='center', color='white')\n",
    "        elif (i, j) == START:\n",
    "            text = ax.text(j, i, 'start', ha='center', va='center', color='white')\n",
    "\n",
    "# Set the grid\n",
    "ax.set_xticks(np.arange(-.5, BOARD_COLS, 1), minor=False)\n",
    "ax.set_yticks(np.arange(-.5, BOARD_ROWS, 1), minor=False)\n",
    "ax.grid(color=\"black\", linestyle='-', linewidth=0.5)\n",
    "ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "# Remove the tick labels\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and Agent Classes with their methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "\n",
    "    def __init__(self, state=START):\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[1, 1] = -1\n",
    "        self.state = state\n",
    "        self.isEnd = False\n",
    "        self.determine = DETERMINISTIC\n",
    "\n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 1\n",
    "        elif self.state == LOSE_STATE:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def isEndFunc(self):\n",
    "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
    "            self.isEnd = True\n",
    "\n",
    "    def nxtPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        -------------\n",
    "        0 | 1 | 2| 3|\n",
    "        1 |\n",
    "        2 |\n",
    "        return next position\n",
    "        \"\"\"\n",
    "        if self.determine:\n",
    "            if action == \"up\":\n",
    "                nxtState = (self.state[0] - 1, self.state[1])\n",
    "            elif action == \"down\":\n",
    "                nxtState = (self.state[0] + 1, self.state[1])\n",
    "            elif action == \"left\":\n",
    "                nxtState = (self.state[0], self.state[1] - 1)\n",
    "            else:\n",
    "                nxtState = (self.state[0], self.state[1] + 1)\n",
    "            # if next state legal\n",
    "            if (nxtState[0] >= 0) and (nxtState[0] <= (BOARD_ROWS -1)):\n",
    "                if (nxtState[1] >= 0) and (nxtState[1] <= (BOARD_COLS -1)):\n",
    "                    if nxtState != (1, 1):\n",
    "                        return nxtState\n",
    "            return self.state\n",
    "\n",
    "    def showBoard(self):\n",
    "        self.board[self.state] = 1\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')\n",
    "\n",
    "\n",
    "# Agent\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.State = State()\n",
    "        self.lr = 0.2 # learning rate\n",
    "        self.exp_rate = 0.3 # exploration rate\n",
    "\n",
    "        # initial state reward\n",
    "        self.state_values = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.state_values[(i, j)] = 0  # set initial value to 0\n",
    "\n",
    "    def chooseAction(self):\n",
    "        # choose action with most expected value\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            for a in self.actions:\n",
    "                # if the action is deterministic\n",
    "                nxt_reward = self.state_values[self.State.nxtPosition(a)]\n",
    "                if nxt_reward >= mx_nxt_reward:\n",
    "                    action = a\n",
    "                    mx_nxt_reward = nxt_reward\n",
    "        return action\n",
    "\n",
    "    def takeAction(self, action):\n",
    "        position = self.State.nxtPosition(action)\n",
    "        return State(state=position)\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.State = State()\n",
    "\n",
    "    def play(self, rounds=10):\n",
    "        i = 0\n",
    "        while i < rounds:\n",
    "            # to the end of game back propagate reward\n",
    "            if self.State.isEnd:\n",
    "                # back propagate\n",
    "                reward = self.State.giveReward()\n",
    "                # explicitly assign end state to reward values\n",
    "                self.state_values[self.State.state] = reward  # this is optional\n",
    "                print(\"Game End Reward\", reward)\n",
    "                for s in reversed(self.states):\n",
    "                    reward = self.state_values[s] + self.lr * (reward - self.state_values[s])\n",
    "                    self.state_values[s] = round(reward, 3)\n",
    "                self.reset()\n",
    "                i += 1\n",
    "            else:\n",
    "                action = self.chooseAction()\n",
    "                # append trace\n",
    "                self.states.append(self.State.nxtPosition(action))\n",
    "                print(\"current position {} action {}\".format(self.State.state, action))\n",
    "                # by taking the action, it reaches the next state\n",
    "                self.State = self.takeAction(action)\n",
    "                # mark is end\n",
    "                self.State.isEndFunc()\n",
    "                print(\"nxt state\", self.State.state)\n",
    "                # print(\"---------------------\")\n",
    "\n",
    "    def showValues(self):\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('----------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                out += str(self.state_values[(i, j)]).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class instance and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent\n",
    "ag = Agent()\n",
    "\n",
    "# Play the game for 100 rounds\n",
    "ag.play(100)\n",
    "\n",
    "print(ag.showValues())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three lines in the result table represent the state values for a 3x4 grid environment in the agent-based model. Each row corresponds to a different horizontal level in the grid, and each cell within a row represents a specific position or state in that level. The grid layout and the values illustrate how the agent has learned to value each position based on its experiences of rewards and penalties throughout its exploration of the environment.<br>\n",
    "\n",
    "Example output:<br>\n",
    "<table border=\"1\">\n",
    "  <tr>\n",
    "    <td>0.933</td>\n",
    "    <td>0.935</td>\n",
    "    <td>0.920</td>\n",
    "    <td>1.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.931</td>\n",
    "    <td>0</td>\n",
    "    <td>0.729</td>\n",
    "    <td>-1.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.929</td>\n",
    "    <td>0.927</td>\n",
    "    <td>0.911</td>\n",
    "    <td>0.662</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
